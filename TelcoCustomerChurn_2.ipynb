{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11178120,"sourceType":"datasetVersion","datasetId":6976880}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Project Objective: The goal of this project is to predict customer churn based on various features related to customer characteristics and subscription details. By analyzing the dataset, we aim to understand the key factors that influence customer retention and apply machine learning techniques to build a model capable of predicting whether a customer will churn. This can help businesses proactively take actions to retain valuable customers and improve customer satisfaction.","metadata":{}},{"cell_type":"markdown","source":"Import Module Section","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:42.046451Z","iopub.execute_input":"2025-03-26T20:26:42.046740Z","iopub.status.idle":"2025-03-26T20:26:44.101907Z","shell.execute_reply.started":"2025-03-26T20:26:42.046709Z","shell.execute_reply":"2025-03-26T20:26:44.100958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load DS","metadata":{}},{"cell_type":"code","source":"# Load dataset\n#df = pd.read_csv('C:/Users/Administrator/Desktop/AIDE/AIDE506 Advanced Machine Learning Topics/Final Project/Telco-Customer-Churn.csv')\ndf = pd.read_csv(\"/kaggle/input/telcocustomerchurn/Telco-Customer-Churn.csv\")\ndf.sample(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:44.103093Z","iopub.execute_input":"2025-03-26T20:26:44.103531Z","iopub.status.idle":"2025-03-26T20:26:44.216329Z","shell.execute_reply.started":"2025-03-26T20:26:44.103501Z","shell.execute_reply":"2025-03-26T20:26:44.215366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:44.219074Z","iopub.execute_input":"2025-03-26T20:26:44.219590Z","iopub.status.idle":"2025-03-26T20:26:44.248926Z","shell.execute_reply.started":"2025-03-26T20:26:44.219528Z","shell.execute_reply":"2025-03-26T20:26:44.247975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df[\"TotalCharges\"] == ' ']\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf[df[\"TotalCharges\"] == ' ']\ndf['TotalCharges'] = df['TotalCharges'].replace([np.inf, -np.inf], np.nan)  \ndf = df.dropna(subset=['TotalCharges'])  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:44.251333Z","iopub.execute_input":"2025-03-26T20:26:44.251812Z","iopub.status.idle":"2025-03-26T20:26:44.270831Z","shell.execute_reply.started":"2025-03-26T20:26:44.251779Z","shell.execute_reply":"2025-03-26T20:26:44.268675Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the data visualization phase, We created various visualizations to gain a deeper understanding of the data. By exploring different features, we were able to identify which ones had a greater impact on the target variable. This process allowed us to make informed decisions about which features to include in our model.\n\nWe also performed various data preprocessing techniques to ensure that the data was in a suitable format for our model. This included handling missing values, encoding categorical variables, and scaling numerical variables. By performing these preprocessing steps, we were able to improve the performance of our model.\n\nIn the model training phase, We used various machine learning algorithms to train our model. We experimented with different algorithms, such as linear regression, decision trees, and random forests, to find the best one for our data. We also used techniques such as cross-validation and hyperparameter tuning to optimize our model's performance.\n\nIn the model evaluation phase, We evaluated our model's performance using various metrics, such as accuracy, precision, recall, and F1 score. We also used techniques such as cross-validation and hyperparameter tuning to optimize our model's performance.\n\nIn the model deployment phase, We deployed our model to a production environment and tested its performance. We also documented our model's performance and limitations, and made any necessary adjustments to improve its performance.\n\nOverall, this project allowed us to gain a deeper understanding of the data and develop a robust machine learning model that could be used to make accurate predictions.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:44.272066Z","iopub.execute_input":"2025-03-26T20:26:44.272395Z","iopub.status.idle":"2025-03-26T20:26:44.623368Z","shell.execute_reply.started":"2025-03-26T20:26:44.272363Z","shell.execute_reply":"2025-03-26T20:26:44.622479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_numeric_distributions(df):\n\n    # Select only numeric columns\n    numeric_cols = df.select_dtypes(include=['number']).columns\n\n    # Set up the size of the plot grid\n    num_cols = len(numeric_cols)\n    num_rows = (num_cols // 3) + 1  # 3 plots per row\n\n    # Create a figure with subplots\n    plt.figure(figsize=(15, 5 * num_rows))\n\n    # Loop through each numeric column and create a distribution plot\n    for i, col in enumerate(numeric_cols, 1):\n        plt.subplot(num_rows, 3, i)\n        sns.histplot(df[col], kde=True, color='blue', bins=30)\n        plt.title(f'Distribution of {col}', fontsize=12)\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n\n    # Adjust layout and show the plots\n    plt.tight_layout()\n    plt.show()\n    \nplot_numeric_distributions(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:44.624309Z","iopub.execute_input":"2025-03-26T20:26:44.624747Z","iopub.status.idle":"2025-03-26T20:26:46.172196Z","shell.execute_reply.started":"2025-03-26T20:26:44.624716Z","shell.execute_reply":"2025-03-26T20:26:46.171330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:46.172978Z","iopub.execute_input":"2025-03-26T20:26:46.173282Z","iopub.status.idle":"2025-03-26T20:26:46.176827Z","shell.execute_reply.started":"2025-03-26T20:26:46.173251Z","shell.execute_reply":"2025-03-26T20:26:46.175916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef plot_categorical_barcharts(df, max_unique=30):\n    # Select categorical columns with less than 15 unique values\n    categorical_columns = [\n        col for col in df.select_dtypes(include=['object', 'category']).columns\n        if df[col].nunique() < max_unique\n    ]\n\n    # Get the number of selected categorical columns\n    num_columns = len(categorical_columns)\n\n    # If no suitable columns are found, print a warning and exit\n    if num_columns == 0:\n        print(f\"No categorical columns with less than {max_unique} unique values.\")\n        return\n\n    # Determine the number of rows and columns for the plot layout\n    num_rows = math.ceil(num_columns / 3)\n    num_cols = min(3, num_columns)\n\n    # Create a grid for subplots\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n    axes = axes.flatten()\n\n    for i, col in enumerate(categorical_columns):\n        # Plot bar chart for each categorical column\n        df[col].value_counts().plot.bar(\n            ax=axes[i],\n            color='lightblue',\n            edgecolor='black'\n        )\n        axes[i].set_title(f'Distribution of {col}')\n        axes[i].set_ylabel('Count')\n        axes[i].set_xlabel('Categories')\n\n    # Remove any extra subplots if the number of columns is not a perfect multiple of 3\n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()\nplot_categorical_barcharts(df, max_unique=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:46.177637Z","iopub.execute_input":"2025-03-26T20:26:46.177847Z","iopub.status.idle":"2025-03-26T20:26:48.587078Z","shell.execute_reply.started":"2025-03-26T20:26:46.177827Z","shell.execute_reply":"2025-03-26T20:26:48.586111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.countplot(x='Churn', data=df, palette='Set2')\n\nplt.title('Distribution of Churn Target Column', fontsize=14)\nplt.xlabel('Churn Status', fontsize=12)\nplt.ylabel('Number of Customers', fontsize=12)\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:48.589064Z","iopub.execute_input":"2025-03-26T20:26:48.589306Z","iopub.status.idle":"2025-03-26T20:26:48.725582Z","shell.execute_reply.started":"2025-03-26T20:26:48.589285Z","shell.execute_reply":"2025-03-26T20:26:48.724909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(x='Contract', hue='Churn', data=df)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:48.726531Z","iopub.execute_input":"2025-03-26T20:26:48.726832Z","iopub.status.idle":"2025-03-26T20:26:48.864201Z","shell.execute_reply.started":"2025-03-26T20:26:48.726798Z","shell.execute_reply":"2025-03-26T20:26:48.863465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot tenure distribution for churned and non-churned customers\nsns.histplot(df[df['Churn'] == 'Yes']['tenure'], color=\"red\", label=\"Churn\", kde=True)\nsns.histplot(df[df['Churn'] == 'No']['tenure'], color=\"blue\", label=\"Not Churn\", kde=True)\n\n# Add legend and title\nplt.legend()\nplt.title(\"Tenure Distribution (Churn vs. Non-Churn Customers)\")\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:48.864971Z","iopub.execute_input":"2025-03-26T20:26:48.865193Z","iopub.status.idle":"2025-03-26T20:26:49.247404Z","shell.execute_reply.started":"2025-03-26T20:26:48.865173Z","shell.execute_reply":"2025-03-26T20:26:49.246628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preprocessing","metadata":{}},{"cell_type":"code","source":"# Drop customer ID\ndf.drop('customerID', axis=1, inplace=True)\n\n# Handle missing values in TotalCharges (coerce to numeric, fill NaNs)\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n\ndf.fillna({'TotalCharges': 0}, inplace=True)\n\n# Convert Churn to binary\ndf['Churn'] = LabelEncoder().fit_transform(df['Churn'])\n\n# Separate features and target\nX = df.drop('Churn', axis=1)\ny = df['Churn']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:49.248111Z","iopub.execute_input":"2025-03-26T20:26:49.248379Z","iopub.status.idle":"2025-03-26T20:26:49.260502Z","shell.execute_reply.started":"2025-03-26T20:26:49.248356Z","shell.execute_reply":"2025-03-26T20:26:49.259776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Encode Categorical Features","metadata":{}},{"cell_type":"code","source":"# Identify categorical columns\ncategorical_cols = [col for col in X.columns if X[col].dtype == 'object']\nbinary_cols = [col for col in categorical_cols if X[col].nunique() == 2]\nmulti_level_cols = list(set(categorical_cols) - set(binary_cols))\n\n# Preprocess binary columns (Label Encoding)\nfor col in binary_cols:\n    X_train[col] = LabelEncoder().fit_transform(X_train[col])\n    X_test[col] = LabelEncoder().fit_transform(X_test[col])\n\n# Preprocess multi-level columns (One-Hot Encoding)\npreprocessor = ColumnTransformer(\n    transformers=[('cat', OneHotEncoder(), multi_level_cols)],\n    remainder='passthrough'\n)\n\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:49.261472Z","iopub.execute_input":"2025-03-26T20:26:49.261786Z","iopub.status.idle":"2025-03-26T20:26:49.315498Z","shell.execute_reply.started":"2025-03-26T20:26:49.261757Z","shell.execute_reply":"2025-03-26T20:26:49.314926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Build Stacked Ensemble Model\n\nDefine Base and Meta Models","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nprint('Building stacked model: Base models -> Meta model')\n# Base models\nbase_models = [\n    ('rf', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)),\n    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n    ('svc', SVC(probability=True, class_weight='balanced', random_state=42))\n]\n# Meta model\nmeta_model = LogisticRegression(class_weight='balanced')\n\n# Stacked model\nstacked_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=meta_model,\n    cv=5,\n    stack_method='predict_proba',\n    n_jobs=-1,\n    verbose=1\n)\nprint('Finsihed Building stacked model!')\n# Train the model\nstacked_model.fit(X_train_processed, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:49.316176Z","iopub.execute_input":"2025-03-26T20:26:49.316446Z","iopub.status.idle":"2025-03-26T20:27:27.082054Z","shell.execute_reply.started":"2025-03-26T20:26:49.316417Z","shell.execute_reply":"2025-03-26T20:27:27.081192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluate Model Performance","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score\n\n# Predictions\ny_pred = stacked_model.predict(X_test_processed)\ny_proba = stacked_model.predict_proba(X_test_processed)[:, 1]\n\n# Evaluation\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\nprint(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:27:27.083016Z","iopub.execute_input":"2025-03-26T20:27:27.083348Z","iopub.status.idle":"2025-03-26T20:27:27.923893Z","shell.execute_reply.started":"2025-03-26T20:27:27.083314Z","shell.execute_reply":"2025-03-26T20:27:27.923096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Generate SHAP Explanations \nCompute SHAP Values","metadata":{}},{"cell_type":"code","source":"import shap\n\n# Sample background and test data for efficiency\nbackground = shap.utils.sample(X_train_processed, 100)\ntest_sample = shap.utils.sample(X_test_processed, 100)\n\n# Initialize Kernel Explainer\nexplainer = shap.KernelExplainer(\n    model=stacked_model.predict_proba,\n    data=background,\n    link='logit',\n    algorithm= \"auto\",\n    verbose=True\n)\nprint(f\"Start calculating Shap Values\")\n# Calculate SHAP values\nshap_values = explainer.shap_values(test_sample,nsamples=500)\nprint(f\"Finished calculating Shap Values\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\n\n# Check if TreeExplainer can be used\nif hasattr(stacked_model, \"predict_proba\"):\n    try:\n        # Use TreeExplainer for faster GPU processing\n        explainer = shap.TreeExplainer(stacked_model)\n        print(\"Using TreeExplainer for optimized SHAP calculations.\")\n    except:\n        # Fall back to KernelExplainer if TreeExplainer is not compatible\n        explainer = shap.KernelExplainer(\n            model=stacked_model.predict_proba,\n            data=shap.utils.sample(X_train_processed, 50), \n            link='logit',\n            algorithm= \"auto\",\n            verbose=True\n        )\n        print(\"Using KernelExplainer due to model incompatibility.\")\nelse:\n    explainer = shap.KernelExplainer(\n        model=stacked_model.predict_proba,\n        data=shap.utils.sample(X_train_processed, 50),  \n        link='logit'\n    )\n    print(\"Using KernelExplainer due to predict_proba absence.\")\n\nprint(f\"Start calculating SHAP values. \")\n\n# Calculate SHAP values with limited samples\nshap_values = explainer.shap_values(\n    shap.utils.sample(X_test_processed, 50), nsamples=300 )\n\nprint(f\"Finished calculating SHAP values.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:37:54.020468Z","iopub.execute_input":"2025-03-26T20:37:54.020830Z","iopub.status.idle":"2025-03-26T20:41:37.752772Z","shell.execute_reply.started":"2025-03-26T20:37:54.020802Z","shell.execute_reply":"2025-03-26T20:41:37.751922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualize SHAP Summaries","metadata":{}},{"cell_type":"markdown","source":"What Does SHAP Summary Plot Do?\nIt summarizes the contribution of each feature across all samples.\n\nShows feature importance, indicating which features have the most impact on model predictions.\n\nDemonstrates both the direction (positive or negative impact) and magnitude (strength) of the feature effects.\n\nHigh impact on positive side: The feature increases the prediction probability.\n\nHigh impact on the negative side: The feature decreases the prediction probability.\n\nDense clustering of dots: Indicates strong, consistent impact.\n\nWider spread: Indicates varying influence depending on the instance.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Ensure test_sample is a DataFrame\nif isinstance(test_sample, np.ndarray):\n    test_sample_df = pd.DataFrame(\n        test_sample,\n        columns=preprocessor.get_feature_names_out()\n    )\nelse:\n    test_sample_df = test_sample\n\nfeature_names = preprocessor.get_feature_names_out()\nclass_idx = 1  # Class index for positive class\n\n# Extract SHAP values for the selected class, considering list or array structure\nif isinstance(shap_values, list):\n    shap_values_class1 = shap_values[class_idx]\nelse:\n    shap_values_class1 = shap_values[:, :, class_idx]\n\n# Adjust the test sample to match the number of SHAP value rows\nnum_shap_rows = shap_values_class1.shape[0]\ntest_sample_df = test_sample_df.iloc[:num_shap_rows]  # Adjust test data to SHAP values\n\n# Verify alignment before plotting\nprint(\"Adjusted Test sample shape:\", test_sample_df.shape)\nprint(\"Adjusted SHAP values shape:\", shap_values_class1.shape)\n\n# SHAP summary plot\nshap.summary_plot(\n    shap_values_class1,\n    test_sample_df,\n    feature_names=feature_names,\n    plot_type='dot',\n    show=False\n)\n\nplt.xlabel(\"SHAP Value (Impact on Model Output)\")\nplt.ylabel(\"Features\")\nplt.title(f\"SHAP Summary Plot for Class {class_idx}\", fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Force plot for the first sample\nshap.force_plot(\n    explainer.expected_value[class_idx],\n    shap_values_class1[0],\n    features=test_sample_df.iloc[0].values,\n    feature_names=feature_names,\n    matplotlib=True,\n    link=\"identity\",\n    figsize=(30, 5),\n    contribution_threshold=0.1\n)\n\n# Interactive force plot for all samples\nshap.initjs()\nshap.force_plot(\n    explainer.expected_value[class_idx],\n    shap_values_class1,\n    test_sample_df\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:55:01.343207Z","iopub.execute_input":"2025-03-26T20:55:01.343530Z","iopub.status.idle":"2025-03-26T20:55:02.613094Z","shell.execute_reply.started":"2025-03-26T20:55:01.343506Z","shell.execute_reply":"2025-03-26T20:55:02.612239Z"}},"outputs":[],"execution_count":null}]}